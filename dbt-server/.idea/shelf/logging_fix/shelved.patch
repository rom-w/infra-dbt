Index: dbt_server/server.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Need to run this as early in the server's startup as possible\nimport os\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom dbt_server import tracer  # noqa\n\nfrom dbt_server import models\nfrom dbt_server.database import engine\nfrom dbt_server.services import dbt_service, filesystem_service\nfrom dbt_server.views import app\nfrom dbt_server.logging import GLOBAL_LOGGER as logger, configure_uvicorn_access_log\nfrom dbt_server.state import LAST_PARSED\nfrom dbt_server.exceptions import StateNotFoundException\n\n\nmodels.Base.metadata.create_all(bind=engine)\ndbt_service.disable_tracking()\n\n\nclass ConfigArgs(BaseModel):\n    target: Optional[str] = None\n    profile: Optional[str] = None\n\n\ndef startup_cache_initialize():\n    \"\"\"\n    Initialize the manifest cache at startup. The cache will only be populated if there is\n    a latest-state-id.txt file pointing to a state folder with a pre-compiled manifest.\n    If any step fails (the latest-state-id.txt file is missing, there's no compiled manifest,\n    or it can't be deserialized) then continue without caching.\n    \"\"\"\n\n    # If an exception is raised in this method, the dbt-server will fail to start up.\n    # Be careful here :)\n\n    latest_state_id = filesystem_service.get_latest_state_id(None)\n    if latest_state_id is None:\n        logger.info(\"[STARTUP] No latest state found - not loading manifest into cache\")\n        return\n\n    manifest_path = filesystem_service.get_path(latest_state_id, \"manifest.msgpack\")\n    logger.info(\n        f\"[STARTUP] Loading manifest from file system (state_id={latest_state_id})\"\n    )\n\n    try:\n        manifest = dbt_service.deserialize_manifest(manifest_path)\n    except (TypeError, ValueError) as e:\n        logger.error(f\"[STARTUP] Could not deserialize manifest: {str(e)}\")\n        return\n    except (StateNotFoundException):\n        logger.error(\n            f\"[STARTUP] Specified latest state not found - not loading manifest (state_id={latest_state_id})\"\n        )\n        return\n\n    target_name = os.environ.get(\"__DBT_TARGET_NAME\", None)\n    if target_name == \"\":\n        target_name = None\n    config_args = ConfigArgs(target=target_name)\n\n    source_path = filesystem_service.get_root_path(latest_state_id)\n    manifest_size = filesystem_service.get_size(manifest_path)\n    config = dbt_service.create_dbt_config(source_path, config_args)\n\n    LAST_PARSED.set_last_parsed_manifest(\n        latest_state_id, manifest, manifest_size, config\n    )\n\n    logger.info(f\"[STARTUP] Cached manifest in memory (state_id={latest_state_id})\")\n\n\n@tracer.wrap\n@app.on_event(\"startup\")\nasync def startup_event():\n    # This method is `async` intentionally to block serving until startup is complete\n    configure_uvicorn_access_log()\n    dbt_service.inject_dd_trace_into_core_lib()\n    startup_cache_initialize()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/server.py b/dbt_server/server.py
--- a/dbt_server/server.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/server.py	(date 1675588807691)
@@ -8,7 +8,7 @@
 from dbt_server.database import engine
 from dbt_server.services import dbt_service, filesystem_service
 from dbt_server.views import app
-from dbt_server.logging import GLOBAL_LOGGER as logger, configure_uvicorn_access_log
+from dbt_server._logging import GLOBAL_LOGGER as logger, configure_uvicorn_access_log
 from dbt_server.state import LAST_PARSED
 from dbt_server.exceptions import StateNotFoundException
 
Index: dbt_server/services/task_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import uuid\n\ntry:\n    from dbt.exceptions import RuntimeException\nexcept (ModuleNotFoundError, ImportError):\n    from dbt.exceptions import DbtRuntimeError as RuntimeException\n\n\nfrom dbt_server import crud, schemas\nfrom dbt_server.services import dbt_service, filesystem_service\nfrom dbt_server.logging import GLOBAL_LOGGER as logger, ServerLog\nfrom dbt_server.models import TaskState\n\nfrom fastapi import HTTPException\nimport asyncio\nimport io\n\n\ndef run_task(task_name, task_id, args, db):\n    db_task = crud.get_task(db, task_id)\n\n    path = filesystem_service.get_root_path(args.state_id)\n    serialize_path = filesystem_service.get_path(args.state_id, \"manifest.msgpack\")\n    # log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    # log_manager = LogManager(log_path)\n\n    # TODO: Structured logging doesn't have the concept of custom log lines like this,\n    # need to follow up with core about a way to do this\n    logger.info(f\"Running dbt ({task_id}) - deserializing manifest {serialize_path}\")\n\n    manifest = dbt_service.deserialize_manifest(serialize_path)\n\n    crud.set_task_running(db, db_task)\n\n    logger.info(f\"Running dbt ({task_id}) - kicking off task\")\n\n    try:\n        if task_name == \"run\":\n            dbt_service.dbt_run(path, args, manifest)\n        elif task_name == \"seed\":\n            dbt_service.dbt_seed(path, args, manifest)\n        elif task_name == \"test\":\n            dbt_service.dbt_test(path, args, manifest)\n        elif task_name == \"build\":\n            dbt_service.dbt_build(path, args, manifest)\n        elif task_name == \"snapshot\":\n            dbt_service.dbt_snapshot(path, args, manifest)\n        elif task_name == \"run_operation\":\n            dbt_service.dbt_run_operation(path, args, manifest)\n        else:\n            raise RuntimeException(\"Not an actual task\")\n    except RuntimeException as e:\n        crud.set_task_errored(db, db_task, str(e))\n        # log_manager.cleanup()\n        raise e\n\n    logger.info(f\"Running dbt ({task_id}) - done\")\n\n    # log_manager.cleanup()\n\n    crud.set_task_done(db, db_task)\n\n\ndef run_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id, state=TaskState.PENDING, command=\"dbt run\", log_path=log_path\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"run\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\ndef test_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id, state=TaskState.PENDING, command=\"dbt test\", log_path=log_path\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"test\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\ndef seed_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id, state=TaskState.PENDING, command=\"dbt seed\", log_path=log_path\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"seed\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\ndef build_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id, state=TaskState.PENDING, command=\"dbt build\", log_path=log_path\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"build\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\ndef run_operation_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id,\n        state=TaskState.PENDING,\n        command=\"dbt run-operation\",\n        log_path=log_path,\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"run_operation\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\ndef snapshot_async(background_tasks, db, args):\n    task_id = str(uuid.uuid4())\n    log_path = filesystem_service.get_path(args.state_id, task_id, \"logs.stdout\")\n\n    task = schemas.Task(\n        task_id=task_id,\n        state=TaskState.PENDING,\n        command=\"dbt snapshot\",\n        log_path=log_path,\n    )\n\n    db_task = crud.get_task(db, task_id)\n    if db_task:\n        raise HTTPException(status_code=400, detail=\"Task already registered\")\n\n    background_tasks.add_task(run_task, \"snapshot\", task_id, args, db)\n    return crud.create_task(db, task)\n\n\nasync def _wait_for_file(path):\n    for _ in range(10):\n        try:\n            return open(path)\n        except FileNotFoundError:\n            # TODO : Remove / debugging\n            logger.info(f\"Waiting for file handle @ {path}\")\n            await asyncio.sleep(0.5)\n            continue\n    else:\n        raise RuntimeError(\"No log file appeared in designated timeout\")\n\n\nasync def _read_until_empty(fh):\n    while True:\n        line = fh.readline()\n        if len(line) == 0:\n            break\n        else:\n            yield line\n\n\nasync def tail_logs_for_path(db, task_id, request, live=True):\n    db_task = crud.get_task(db, task_id)\n    logger.info(f\"Waiting for file @ {db_task.log_path}\")\n    fh = await _wait_for_file(db_task.log_path)\n\n    if live:\n        fh.seek(0, io.SEEK_END)\n    try:\n        while db_task.state not in (TaskState.ERROR, TaskState.FINISHED):\n            if await request.is_disconnected():\n                logger.debug(\"Log request disconnected\")\n                break\n            async for log in _read_until_empty(fh):\n                yield log\n            await asyncio.sleep(0.5)\n            db.refresh(db_task)\n\n        # Drain any lines accumulated after end of task\n        # If we didn't do this, some lines could be omitted\n        logger.info(\"Draining logs from file\")\n        async for log in _read_until_empty(fh):\n            yield log\n\n    finally:\n        yield ServerLog(state=db_task.state, error=db_task.error).to_json()\n        fh.close()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/services/task_service.py b/dbt_server/services/task_service.py
--- a/dbt_server/services/task_service.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/services/task_service.py	(date 1675588807710)
@@ -8,7 +8,7 @@
 
 from dbt_server import crud, schemas
 from dbt_server.services import dbt_service, filesystem_service
-from dbt_server.logging import GLOBAL_LOGGER as logger, ServerLog
+from dbt_server._logging import GLOBAL_LOGGER as logger, ServerLog
 from dbt_server.models import TaskState
 
 from fastapi import HTTPException
Index: dbt_server/services/dbt_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport threading\nimport uuid\nfrom inspect import getmembers, isfunction\n\n# dbt Core imports\nimport dbt.tracking\nimport dbt.lib\nimport dbt.adapters.factory\n\n\n# These exceptions were removed in v1.4\ntry:\n    from dbt.exceptions import (\n        ValidationException,\n        CompilationException,\n        InvalidConnectionException,\n    )\nexcept (ModuleNotFoundError, ImportError):\n    from dbt.exceptions import (\n        DbtValidationError as ValidationException,\n        CompilationError as CompilationException,\n        InvalidConnectionError as InvalidConnectionException,\n    )\n\nfrom dbt.lib import (\n    create_task,\n    get_dbt_config as dbt_get_dbt_config,\n    parse_to_manifest as dbt_parse_to_manifest,\n    execute_sql as dbt_execute_sql,\n    deserialize_manifest as dbt_deserialize_manifest,\n    serialize_manifest as dbt_serialize_manifest,\n    SqlCompileRunnerNoIntrospection,\n)\n\n\nfrom dbt.parser.sql import SqlBlockParser\nfrom dbt.parser.manifest import process_node\n\nfrom dbt.contracts.sql import (\n    RemoteRunResult,\n    RemoteCompileResult,\n)\n\n# dbt Server imports\nfrom dbt_server.services import filesystem_service\nfrom dbt_server import tracer\nfrom dbt_server.logging import GLOBAL_LOGGER as logger\n\nfrom dbt_server.exceptions import (\n    InvalidConfigurationException,\n    InternalException,\n    dbtCoreCompilationException,\n    UnsupportedQueryException,\n)\nfrom dbt_server.helpers import set_profile_name\n\nALLOW_INTROSPECTION = str(os.environ.get(\"__DBT_ALLOW_INTROSPECTION\", \"1\")).lower() in (\n    \"true\",\n    \"1\",\n    \"on\",\n)\n\nCONFIG_GLOBAL_LOCK = threading.Lock()\n\n\ndef inject_dd_trace_into_core_lib():\n\n    for attr_name, attr in getmembers(dbt.lib):\n        if not isfunction(attr):\n            continue\n\n        setattr(dbt.lib, attr_name, tracer.wrap(attr))\n\n\ndef handle_dbt_compilation_error(func):\n    def inner(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logger.exception(\"Unhandled error from dbt Core\")\n            raise dbtCoreCompilationException(str(e))\n\n    return inner\n\n\ndef patch_adapter_config(config):\n    # This is a load-bearing assignment. Adapters cache the config they are\n    # provided (oh my!) and they are represented as singletons, so it's not\n    # actually possible to invoke dbt twice concurrently with two different\n    # configs that differ substantially. Fortunately, configs _mostly_ carry\n    # credentials. The risk here is around changes to other config-type code,\n    # like packages.yml contents or the name of the project.\n    #\n    # This quickfix is intended to support sequential requests with\n    # differing project names. It will _not_ work with concurrent requests, as one\n    # of those requests will get a surprising project_name in the adapter's\n    # cached RuntimeConfig object.\n    #\n    # If this error is hit in practice, it will manifest as something like:\n    #      Node package named fishtown_internal_analytics not found!\n    #\n    # because Core is looking for packages in the wrong namespace. This\n    # unfortunately isn't something we can readily catch programmatically\n    # & it must be fixed upstream in dbt Core.\n    adapter = dbt.adapters.factory.get_adapter(config)\n    adapter.config = config\n    return adapter\n\n\ndef generate_node_name():\n    return str(uuid.uuid4()).replace(\"-\", \"_\")\n\n\n@tracer.wrap\ndef get_sql_parser(config, manifest):\n    return SqlBlockParser(\n        project=config,\n        manifest=manifest,\n        root_project=config,\n    )\n\n\n@tracer.wrap\ndef create_dbt_config(project_path, args=None):\n    try:\n        args = set_profile_name(args)\n        # This needs a lock to prevent two threads from mutating an adapter concurrently\n        with CONFIG_GLOBAL_LOCK:\n            return dbt_get_dbt_config(project_path, args)\n    except ValidationException:\n        # Some types of dbt config exceptions may contain sensitive information\n        # eg. contents of a profiles.yml file for invalid/malformed yml.\n        # Raise a new exception to mask the original backtrace and suppress\n        # potentially sensitive information.\n        raise InvalidConfigurationException(\n            \"Invalid dbt config provided. Check that your credentials are configured\"\n            \" correctly and a valid dbt project is present\"\n        )\n\n\ndef disable_tracking():\n    # TODO: why does this mess with stuff\n\n    dbt.tracking.disable_tracking()\n\n\n@tracer.wrap\ndef parse_to_manifest(project_path, args):\n    try:\n        config = create_dbt_config(project_path, args)\n        patch_adapter_config(config)\n        return dbt_parse_to_manifest(config)\n    except CompilationException as e:\n        logger.error(\n            f\"Failed to parse manifest at {project_path}. Compilation Error: {repr(e)}\"\n        )\n        raise dbtCoreCompilationException(e)\n\n\n@tracer.wrap\ndef serialize_manifest(manifest, serialize_path):\n    manifest_msgpack = dbt_serialize_manifest(manifest)\n    filesystem_service.write_file(serialize_path, manifest_msgpack)\n\n\n@tracer.wrap\ndef deserialize_manifest(serialize_path):\n    manifest_packed = filesystem_service.read_serialized_manifest(serialize_path)\n    return dbt_deserialize_manifest(manifest_packed)\n\n\ndef dbt_run(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"run\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_test(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"test\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_list(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"list\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_seed(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"seed\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_build(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"build\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_run_operation(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"run_operation\", args, manifest, config)\n    return task.run()\n\n\ndef dbt_snapshot(project_path, args, manifest):\n    config = create_dbt_config(project_path, args)\n    task = create_task(\"snapshot\", args, manifest, config)\n    return task.run()\n\n\n@handle_dbt_compilation_error\n@tracer.wrap\ndef execute_sql(manifest, project_path, sql):\n    try:\n        node_name = generate_node_name()\n        result = dbt_execute_sql(manifest, project_path, sql, node_name)\n    except CompilationException as e:\n        logger.error(\n            f\"Failed to compile sql at {project_path}. Compilation Error: {repr(e)}\"\n        )\n        raise dbtCoreCompilationException(e)\n\n    if type(result) != RemoteRunResult:\n        # Theoretically this shouldn't happen-- handling just in case\n        raise InternalException(\n            f\"Got unexpected result type ({type(result)}) from dbt Core\"\n        )\n\n    return result.to_dict()\n\n\n@handle_dbt_compilation_error\n@tracer.wrap\ndef compile_sql(manifest, config, parser, sql):\n    try:\n        node_name = generate_node_name()\n        adapter = patch_adapter_config(config)\n\n        sql_node = parser.parse_remote(sql, node_name)\n        process_node(config, manifest, sql_node)\n\n        runner = SqlCompileRunnerNoIntrospection(config, adapter, sql_node, 1, 1)\n        result = runner.safe_run(manifest)\n\n    except InvalidConnectionException:\n        if ALLOW_INTROSPECTION:\n            # Raise original error if introspection is not disabled\n            # and therefore errors are unexpected\n            raise\n        else:\n            msg = \"This dbt server environment does not support introspective queries. \\n Hint: typically introspective queries use the 'run_query' jinja command or a macro that invokes that command\"\n            logger.exception(msg)\n            raise UnsupportedQueryException(msg)\n    except CompilationException as e:\n        logger.error(f\"Failed to compile sql. Compilation Error: {repr(e)}\")\n        raise dbtCoreCompilationException(e)\n\n    if type(result) != RemoteCompileResult:\n        # Theoretically this shouldn't happen-- handling just in case\n        raise InternalException(\n            f\"Got unexpected result type ({type(result)}) from dbt Core\"\n        )\n\n    return result.to_dict()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/services/dbt_service.py b/dbt_server/services/dbt_service.py
--- a/dbt_server/services/dbt_service.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/services/dbt_service.py	(date 1675588771615)
@@ -45,7 +45,7 @@
 # dbt Server imports
 from dbt_server.services import filesystem_service
 from dbt_server import tracer
-from dbt_server.logging import GLOBAL_LOGGER as logger
+from dbt_server._logging import GLOBAL_LOGGER as logger
 
 from dbt_server.exceptions import (
     InvalidConfigurationException,
Index: dbt_server/views.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from collections import deque\nimport dbt.events.functions\nimport os\nimport signal\n\nfrom sse_starlette.sse import EventSourceResponse\nfrom fastapi import FastAPI, BackgroundTasks, Depends, status\nfrom fastapi.exceptions import RequestValidationError\nfrom starlette.requests import Request\nfrom pydantic import BaseModel, Field\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import JSONResponse\nfrom typing import List, Optional, Union, Dict\n\nfrom dbt_server.state import StateController\nfrom dbt_server import crud, schemas, helpers\nfrom dbt_server import tracer\n\nfrom dbt_server.services import (\n    filesystem_service,\n    dbt_service,\n    task_service,\n)\n\nfrom dbt_server.exceptions import (\n    InvalidConfigurationException,\n    InvalidRequestException,\n    InternalException,\n    StateNotFoundException,\n)\nfrom dbt_server.logging import GLOBAL_LOGGER as logger\n\n# ORM stuff\nfrom sqlalchemy.orm import Session\n\n# We need to override the EVENT_HISTORY queue to store\n# only a small amount of events to prevent too much memory\n# from being used.\ndbt.events.functions.EVENT_HISTORY = deque(maxlen=10)\n\n\n# Enable `ALLOW_ORCHESTRATED_SHUTDOWN` to instruct dbt server to\n# ignore a first SIGINT or SIGTERM and enable a `/shutdown` endpoint\nALLOW_ORCHESTRATED_SHUTDOWN = os.environ.get(\n    \"ALLOW_ORCHESTRATED_SHUTDOWN\", \"0\"\n).lower() in (\"true\", \"1\", \"on\")\n\napp = FastAPI()\n\n\n@app.middleware(\"http\")\nasync def log_request_start(request: Request, call_next):\n    logger.debug(f\"Received request: {request.method} {request.url.path}\")\n    response = await call_next(request)\n    return response\n\n\nclass FileInfo(BaseModel):\n    contents: str\n    hash: str\n    path: str\n\n\nclass PushProjectArgs(BaseModel):\n    state_id: str\n    body: Dict[str, FileInfo]\n\n\nclass ParseArgs(BaseModel):\n    state_id: str\n    version_check: Optional[bool] = None\n    profile: Optional[str] = None\n    target: Optional[str] = None\n\n\nclass BuildArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    resource_types: Optional[List[str]] = None\n    select: Union[None, str, List[str]] = None\n    threads: Optional[int] = None\n    exclude: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    state: Optional[str] = None\n    defer: Optional[bool] = None\n    fail_fast: Optional[bool] = None\n    full_refresh: Optional[bool] = None\n    store_failures: Optional[bool] = None\n    indirect_selection: str = \"\"\n    version_check: Optional[bool] = None\n\n\nclass RunArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    threads: Optional[int] = None\n    models: Union[None, str, List[str]] = None\n    select: Union[None, str, List[str]] = None\n    exclude: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    state: Optional[str] = None\n    defer: Optional[bool] = None\n    fail_fast: Optional[bool] = None\n    full_refresh: Optional[bool] = None\n    version_check: Optional[bool] = None\n\n\nclass TestArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    threads: Optional[int] = None\n    data_type: bool = Field(False, alias=\"data\")\n    schema_type: bool = Field(False, alias=\"schema\")\n    models: Union[None, str, List[str]] = None\n    select: Union[None, str, List[str]] = None\n    exclude: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    state: Optional[str] = None\n    defer: Optional[bool] = None\n    fail_fast: Optional[bool] = None\n    store_failures: Optional[bool] = None\n    full_refresh: Optional[bool] = None\n    indirect_selection: str = \"\"\n    version_check: Optional[bool] = None\n\n\nclass SeedArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    threads: Optional[int] = None\n    models: Union[None, str, List[str]] = None\n    select: Union[None, str, List[str]] = None\n    exclude: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    show: Optional[bool] = None\n    state: Optional[str] = None\n    selector_name: Optional[str] = None\n    full_refresh: Optional[bool] = None\n    version_check: Optional[bool] = None\n\n\nclass ListArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    resource_types: Optional[List[str]] = None\n    models: Union[None, str, List[str]] = None\n    exclude: Union[None, str, List[str]] = None\n    select: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    output: Optional[str] = \"name\"\n    output_keys: Union[None, str, List[str]] = None\n    state: Optional[str] = None\n    indirect_selection: str = \"eager\"\n\n\nclass SnapshotArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    single_threaded: Optional[bool] = None\n    threads: Optional[int] = None\n    resource_types: Optional[List[str]] = None\n    models: Union[None, str, List[str]] = None\n    select: Union[None, str, List[str]] = None\n    exclude: Union[None, str, List[str]] = None\n    selector_name: Optional[str] = None\n    state: Optional[str] = None\n    defer: Optional[bool] = None\n\n\nclass RunOperationArgs(BaseModel):\n    state_id: str\n    profile: Optional[str] = None\n    target: Optional[str] = None\n    macro: str\n    single_threaded: Optional[bool] = None\n    args: str = Field(default=\"{}\")\n\n\nclass SQLConfig(BaseModel):\n    state_id: Optional[str] = None\n    sql: str\n    target: Optional[str] = None\n    profile: Optional[str] = None\n\n\n@app.exception_handler(InvalidConfigurationException)\nasync def configuration_exception_handler(\n    request: Request, exc: InvalidConfigurationException\n):\n    status_code = status.HTTP_422_UNPROCESSABLE_ENTITY\n    exc_str = f\"{exc}\".replace(\"\\n\", \" \").replace(\"   \", \" \")\n    logger.error(f\"Request to {request.url} failed validation: {exc_str}\")\n    content = {\"status_code\": status_code, \"message\": exc_str, \"data\": None}\n    return JSONResponse(content=content, status_code=status_code)\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    status_code = status.HTTP_422_UNPROCESSABLE_ENTITY\n    exc_str = f\"{exc}\".replace(\"\\n\", \" \").replace(\"   \", \" \")\n    logger.error(f\"Request to {request.url} failed validation: {exc_str}\")\n    content = {\"status_code\": status_code, \"message\": exc_str, \"data\": None}\n    return JSONResponse(content=content, status_code=status_code)\n\n\n@app.exception_handler(InternalException)\nasync def unhandled_internal_error(request: Request, exc: InternalException):\n    status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n    exc_str = f\"{exc}\".replace(\"\\n\", \" \").replace(\"   \", \" \")\n    logger.error(f\"Request to {request.url} failed with an internal error: {exc_str}\")\n\n    content = {\"status_code\": status_code, \"message\": exc_str, \"data\": None}\n    return JSONResponse(content=content, status_code=status_code)\n\n\n@app.exception_handler(InvalidRequestException)\nasync def handled_dbt_error(request: Request, exc: InvalidRequestException):\n    # Missing states get a 422, otherwise they get a 400\n    if isinstance(exc, StateNotFoundException):\n        status_code = status.HTTP_422_UNPROCESSABLE_ENTITY\n    else:\n        status_code = status.HTTP_400_BAD_REQUEST\n\n    exc_str = f\"{exc}\".replace(\"\\n\", \" \").replace(\"   \", \" \")\n    logger.error(f\"Request to {request.url} was invalid: {exc_str}\")\n\n    content = {\"status_code\": status_code, \"message\": exc_str, \"data\": None}\n    return JSONResponse(content=content, status_code=status_code)\n\n\nif ALLOW_ORCHESTRATED_SHUTDOWN:\n\n    @app.post(\"/shutdown\")\n    async def shutdown():\n        # raise 2 SIGTERM signals, just to\n        # make sure this really shuts down.\n        # raising a SIGKILL logs some\n        # warnings about leaked semaphores\n        signal.raise_signal(signal.SIGTERM)\n        signal.raise_signal(signal.SIGTERM)\n        return JSONResponse(\n            status_code=200,\n            content={},\n        )\n\n\n@app.post(\"/ready\")\nasync def ready():\n    return JSONResponse(status_code=200, content={})\n\n\n@app.post(\"/push\")\ndef push_unparsed_manifest(args: PushProjectArgs):\n    # Parse / validate it\n    state_id = filesystem_service.get_latest_state_id(args.state_id)\n\n    size_in_files = len(args.body)\n    size_in_bytes = sum(len(file.contents) for file in args.body.values())\n    logger.info(f\"Recieved manifest {size_in_files} files, {size_in_bytes} bytes\")\n\n    path = filesystem_service.get_root_path(state_id)\n    reuse = True\n\n    # Stupid example of reusing an existing manifest\n    if not os.path.exists(path):\n        reuse = False\n        filesystem_service.write_unparsed_manifest_to_disk(state_id, args.body)\n\n    # Write messagepack repr to disk\n    # Return a key that the client can use to operate on it?\n    return JSONResponse(\n        status_code=200,\n        content={\n            \"state\": state_id,\n            \"bytes\": len(args.body),\n            \"reuse\": reuse,\n            \"path\": path,\n        },\n    )\n\n\n@app.post(\"/parse\")\ndef parse_project(args: ParseArgs):\n    state = StateController.parse_from_source(args.state_id, args)\n    state.serialize_manifest()\n    state.update_state_id()\n    state.update_cache()\n\n    tracer.add_tags_to_current_span({\"manifest_size\": state.manifest_size})\n\n    return JSONResponse(\n        status_code=200,\n        content={\"parsing\": args.state_id, \"path\": state.serialize_path},\n    )\n\n\n@app.post(\"/run\")\nasync def run_models(args: RunArgs):\n    state_id = filesystem_service.get_latest_state_id(args.state_id)\n    path = filesystem_service.get_root_path(state_id)\n    serialize_path = filesystem_service.get_path(state_id, \"manifest.msgpack\")\n\n    manifest = dbt_service.deserialize_manifest(serialize_path)\n    results = dbt_service.dbt_run(path, args, manifest)\n    encoded_results = jsonable_encoder(results.to_dict())\n    return JSONResponse(\n        status_code=200,\n        content={\n            \"parsing\": args.state_id,\n            \"path\": serialize_path,\n            \"res\": encoded_results,\n        },\n    )\n\n\n@app.post(\"/list\")\nasync def list_resources(args: ListArgs):\n    state_id = filesystem_service.get_latest_state_id(args.state_id)\n    path = filesystem_service.get_root_path(state_id)\n    serialize_path = filesystem_service.get_path(state_id, \"manifest.msgpack\")\n\n    manifest = dbt_service.deserialize_manifest(serialize_path)\n    results = dbt_service.dbt_list(path, args, manifest)\n\n    encoded_results = jsonable_encoder(results)\n\n    return JSONResponse(\n        status_code=200,\n        content={\n            \"parsing\": args.state_id,\n            \"path\": serialize_path,\n            \"res\": encoded_results,\n        },\n    )\n\n\n@app.post(\"/run-async\")\nasync def run_models_async(\n    args: RunArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.run_async(background_tasks, db, args)\n\n\n@app.post(\"/test-async\")\nasync def test_async(\n    args: TestArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.test_async(background_tasks, db, args)\n\n\n@app.post(\"/seed-async\")\nasync def seed_async(\n    args: SeedArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.seed_async(background_tasks, db, args)\n\n\n@app.post(\"/build-async\")\nasync def build_async(\n    args: BuildArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.build_async(background_tasks, db, args)\n\n\n@app.post(\"/snapshot-async\")\nasync def snapshot_async(\n    args: SnapshotArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.snapshot_async(background_tasks, db, args)\n\n\n@app.post(\"/run-operation-async\")\nasync def run_operation_async(\n    args: RunOperationArgs,\n    background_tasks: BackgroundTasks,\n    response_model=schemas.Task,\n    db: Session = Depends(crud.get_db),\n):\n    return task_service.run_operation_async(background_tasks, db, args)\n\n\n@app.post(\"/preview\")\nasync def preview_sql(sql: SQLConfig):\n    state = StateController.load_state(sql.state_id, sql)\n    result = state.execute_query(sql.sql)\n    compiled_code = helpers.extract_compiled_code_from_node(result)\n\n    tag_request_span(state)\n    return JSONResponse(\n        status_code=200,\n        content={\n            \"parsing\": state.state_id,\n            \"path\": state.serialize_path,\n            \"res\": jsonable_encoder(result),\n            \"compiled_code\": compiled_code,\n        },\n    )\n\n\n@app.post(\"/compile\")\ndef compile_sql(sql: SQLConfig):\n    state = StateController.load_state(sql.state_id, sql)\n    result = state.compile_query(sql.sql)\n    compiled_code = helpers.extract_compiled_code_from_node(result)\n\n    tag_request_span(state)\n\n    return JSONResponse(\n        status_code=200,\n        content={\n            \"parsing\": state.state_id,\n            \"path\": state.serialize_path,\n            \"res\": jsonable_encoder(result),\n            \"compiled_code\": compiled_code,\n        },\n    )\n\n\ndef tag_request_span(state):\n    manifest_metadata = get_manifest_metadata(state)\n    tracer.add_tags_to_current_span(manifest_metadata)\n\n\ndef get_manifest_metadata(state):\n    return {\n        \"manifest_size\": state.manifest_size,\n        \"is_manifest_cached\": state.is_manifest_cached,\n    }\n\n\nclass Task(BaseModel):\n    task_id: str\n\n\n@app.get(\"/stream-logs/{task_id}\")\nasync def log_endpoint(\n    task_id: str,\n    request: Request,\n    db: Session = Depends(crud.get_db),\n):\n    event_generator = task_service.tail_logs_for_path(db, task_id, request)\n    return EventSourceResponse(event_generator, ping=2)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/views.py b/dbt_server/views.py
--- a/dbt_server/views.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/views.py	(date 1675588807699)
@@ -28,7 +28,7 @@
     InternalException,
     StateNotFoundException,
 )
-from dbt_server.logging import GLOBAL_LOGGER as logger
+from dbt_server._logging import GLOBAL_LOGGER as logger
 
 # ORM stuff
 from sqlalchemy.orm import Session
Index: dbt_server/services/filesystem_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport shutil\nfrom dbt_server.logging import GLOBAL_LOGGER as logger\nfrom dbt_server.exceptions import StateNotFoundException\nfrom dbt_server import tracer\n\n\ndef get_working_dir():\n    return os.environ.get(\"__DBT_WORKING_DIR\", \"./working-dir\")\n\n\ndef get_root_path(state_id):\n    working_dir = get_working_dir()\n    return os.path.join(working_dir, f\"state-{state_id}\")\n\n\ndef get_latest_state_file_path():\n    working_dir = get_working_dir()\n    return os.path.join(working_dir, \"latest-state-id.txt\")\n\n\ndef get_path(state_id, *path_parts):\n    return os.path.join(get_root_path(state_id), *path_parts)\n\n\n@tracer.wrap\ndef get_size(path):\n    return os.path.getsize(path)\n\n\n@tracer.wrap\ndef ensure_dir_exists(path):\n    dirname = os.path.dirname(path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n\n@tracer.wrap\ndef write_file(path, contents):\n    ensure_dir_exists(path)\n\n    with open(path, \"wb\") as fh:\n        if isinstance(contents, str):\n            contents = contents.encode(\"utf-8\")\n        fh.write(contents)\n\n\n@tracer.wrap\ndef read_serialized_manifest(path):\n    try:\n        with open(path, \"rb\") as fh:\n            return fh.read()\n    except FileNotFoundError as e:\n        raise StateNotFoundException(e)\n\n\n@tracer.wrap\ndef write_unparsed_manifest_to_disk(state_id, filedict):\n    root_path = get_root_path(state_id)\n    if os.path.exists(root_path):\n        shutil.rmtree(root_path)\n\n    for filename, file_info in filedict.items():\n        path = get_path(state_id, filename)\n        write_file(path, file_info.contents)\n\n\n@tracer.wrap\ndef get_latest_state_id(state_id):\n    if not state_id:\n        path = os.path.abspath(get_latest_state_file_path())\n        if not os.path.exists(path):\n            logger.error(\"No state id included in request, no previous state id found.\")\n            return None\n        with open(path, \"r\") as latest_path_file:\n            state_id = latest_path_file.read().strip()\n    return state_id\n\n\n@tracer.wrap\ndef update_state_id(state_id):\n    path = os.path.abspath(get_latest_state_file_path())\n    with open(path, \"w+\") as latest_path_file:\n        latest_path_file.write(state_id)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/services/filesystem_service.py b/dbt_server/services/filesystem_service.py
--- a/dbt_server/services/filesystem_service.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/services/filesystem_service.py	(date 1675588748370)
@@ -1,6 +1,6 @@
 import os
 import shutil
-from dbt_server.logging import GLOBAL_LOGGER as logger
+from dbt_server._logging import GLOBAL_LOGGER as logger
 from dbt_server.exceptions import StateNotFoundException
 from dbt_server import tracer
 
Index: dbt_server/state.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from dbt_server.services import filesystem_service, dbt_service\nfrom dbt_server.exceptions import StateNotFoundException\nfrom dbt_server.logging import GLOBAL_LOGGER as logger\nfrom dbt_server import tracer\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Any\nimport threading\n\n\nMANIFEST_LOCK = threading.Lock()\n\n\n@dataclass\nclass CachedManifest:\n    state_id: Optional[str] = None\n    manifest: Optional[Any] = None\n    manifest_size: Optional[int] = None\n\n    config: Optional[Any] = None\n    parser: Optional[Any] = None\n\n    def set_last_parsed_manifest(self, state_id, manifest, manifest_size, config):\n        with MANIFEST_LOCK:\n            self.state_id = state_id\n            self.manifest = manifest\n            self.manifest_size = manifest_size\n            self.config = config\n\n            self.parser = dbt_service.get_sql_parser(self.config, self.manifest)\n\n    def lookup(self, state_id):\n        with MANIFEST_LOCK:\n            if self.manifest is None:\n                return None\n            elif state_id in (None, self.state_id):\n                return self\n            else:\n                return None\n\n    # used for testing...\n    def reset(self):\n        with MANIFEST_LOCK:\n            self.state_id = None\n            self.manifest = None\n            self.manifest_size = None\n            self.config = None\n            self.parser = None\n\n\nLAST_PARSED = CachedManifest()\n\n\nclass StateController(object):\n    def __init__(\n        self, state_id, manifest, config, parser, manifest_size, is_manifest_cached\n    ):\n        self.state_id = state_id\n        self.manifest = manifest\n        self.config = config\n        self.parser = parser\n        self.manifest_size = manifest_size\n        self.is_manifest_cached = is_manifest_cached\n\n        self.root_path = filesystem_service.get_root_path(state_id)\n        self.serialize_path = filesystem_service.get_path(state_id, \"manifest.msgpack\")\n\n    @classmethod\n    @tracer.wrap\n    def from_parts(cls, state_id, manifest, source_path, manifest_size, args=None):\n        config = dbt_service.create_dbt_config(source_path, args)\n        parser = dbt_service.get_sql_parser(config, manifest)\n\n        return cls(\n            state_id=state_id,\n            manifest=manifest,\n            config=config,\n            parser=parser,\n            manifest_size=manifest_size,\n            is_manifest_cached=False,\n        )\n\n    @classmethod\n    @tracer.wrap\n    def from_cached(cls, cached):\n        return cls(\n            state_id=cached.state_id,\n            manifest=cached.manifest,\n            config=cached.config,\n            parser=cached.parser,\n            manifest_size=cached.manifest_size,\n            is_manifest_cached=True,\n        )\n\n    @classmethod\n    @tracer.wrap\n    def parse_from_source(cls, state_id, parse_args=None):\n        \"\"\"\n        Loads a manifest from source code in a specified directory based on the\n        provided state_id. This method will cache the parsed manifest in memory\n        before returning.\n        \"\"\"\n        source_path = filesystem_service.get_root_path(state_id)\n        logger.info(f\"Parsing manifest from filetree (state_id={state_id})\")\n        manifest = dbt_service.parse_to_manifest(source_path, parse_args)\n\n        logger.info(f\"Done parsing from source (state_id={state_id})\")\n        return cls.from_parts(state_id, manifest, source_path, 0, parse_args)\n\n    @classmethod\n    @tracer.wrap\n    def load_state(cls, state_id, args=None):\n        \"\"\"\n        Loads a manifest given a state_id from an in-memory cache if present,\n        or from disk at a location specified by the state_id argument. The\n        manifest cached in-memory will updated on every /parse request, so\n        state_ids which are None (ie. \"latest\") or exactly matching the latest\n        parsed state_id will be cache hits.\n        \"\"\"\n        cached = LAST_PARSED.lookup(state_id)\n        if cached:\n            logger.info(f\"Loading manifest from cache ({cached.state_id})\")\n            return cls.from_cached(cached)\n\n        # Not in cache - need to go to filesystem to deserialize it\n        logger.info(f\"Manifest cache miss (state_id={state_id})\")\n        state_id = filesystem_service.get_latest_state_id(state_id)\n\n        # No state_id provided, and no latest-state-id.txt found\n        if state_id is None:\n            raise StateNotFoundException(\n                f\"Provided state_id does not exist or is not found ({state_id})\"\n            )\n\n        # Don't cache on deserialize - that's only for /parse\n        manifest_path = filesystem_service.get_path(state_id, \"manifest.msgpack\")\n        logger.info(f\"Loading manifest from file system ({manifest_path})\")\n        manifest = dbt_service.deserialize_manifest(manifest_path)\n        manifest_size = filesystem_service.get_size(manifest_path)\n\n        source_path = filesystem_service.get_root_path(state_id)\n        return cls.from_parts(state_id, manifest, source_path, manifest_size, args)\n\n    @tracer.wrap\n    def serialize_manifest(self):\n        logger.info(f\"Serializing manifest to file system ({self.serialize_path})\")\n        dbt_service.serialize_manifest(self.manifest, self.serialize_path)\n        self.manifest_size = filesystem_service.get_size(self.serialize_path)\n\n    @tracer.wrap\n    def update_state_id(self):\n        logger.info(f\"Updating latest state id ({self.state_id})\")\n        filesystem_service.update_state_id(self.state_id)\n\n    @tracer.wrap\n    def compile_query(self, query):\n        return dbt_service.compile_sql(\n            self.manifest,\n            self.config,\n            self.parser,\n            query,\n        )\n\n    @tracer.wrap\n    def execute_query(self, query):\n        return dbt_service.execute_sql(self.manifest, self.root_path, query)\n\n    @tracer.wrap\n    def update_cache(self):\n        logger.info(f\"Updating cache (state_id={self.state_id})\")\n        LAST_PARSED.set_last_parsed_manifest(\n            self.state_id, self.manifest, self.manifest_size, self.config\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dbt_server/state.py b/dbt_server/state.py
--- a/dbt_server/state.py	(revision 9adeb042979b86120a6a5d428b084fc3588f681d)
+++ b/dbt_server/state.py	(date 1675588807704)
@@ -1,6 +1,6 @@
 from dbt_server.services import filesystem_service, dbt_service
 from dbt_server.exceptions import StateNotFoundException
-from dbt_server.logging import GLOBAL_LOGGER as logger
+from dbt_server._logging import GLOBAL_LOGGER as logger
 from dbt_server import tracer
 
 from dataclasses import dataclass
diff --git a/dbt_server/logging.py b/dbt_server/_logging.py
rename from dbt_server/logging.py
rename to dbt_server/_logging.py
